{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxFz+7osK2L6IQtayc24cR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d6b26eee96364222af8d4559006f1872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea5a04ddf4dc450da9e403b4b947cb55",
              "IPY_MODEL_efa848e16857481d8d2dae84ddb2b4c4",
              "IPY_MODEL_8b7c9f99222b4a3789baad3345c3a39e"
            ],
            "layout": "IPY_MODEL_6ab0f5b729164334a2a6cef4885b1601"
          }
        },
        "ea5a04ddf4dc450da9e403b4b947cb55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d33745f8f9c94113b374c146d2db79ad",
            "placeholder": "​",
            "style": "IPY_MODEL_1a8115de02984c48849bfe417ec59ccb",
            "value": "config.json: 100%"
          }
        },
        "efa848e16857481d8d2dae84ddb2b4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2545f44262264dc49b21d113c3c61aee",
            "max": 1111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c80dcaa7c91941cf87dbdc6b0c2f4fbf",
            "value": 1111
          }
        },
        "8b7c9f99222b4a3789baad3345c3a39e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0bb4436c6fb454e9d21b13c5061bde4",
            "placeholder": "​",
            "style": "IPY_MODEL_1ec61c0d684b4e22acd57c185f2dcec5",
            "value": " 1.11k/1.11k [00:00&lt;00:00, 56.1kB/s]"
          }
        },
        "6ab0f5b729164334a2a6cef4885b1601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d33745f8f9c94113b374c146d2db79ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a8115de02984c48849bfe417ec59ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2545f44262264dc49b21d113c3c61aee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c80dcaa7c91941cf87dbdc6b0c2f4fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0bb4436c6fb454e9d21b13c5061bde4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ec61c0d684b4e22acd57c185f2dcec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinseriouspark/embedding_for_all/blob/main/%5Bw2%5D_fine_tuning_%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning\n",
        "- pretrained model with task"
      ],
      "metadata": {
        "id": "3GuPik-hUJvj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUVHB4LMUHCy",
        "outputId": "1a2045db-934c-4227-cdaf-ef3597aaa5c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->transformers[torch])\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.28.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement accelrate (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for accelrate\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers[torch]\n",
        "!pip install accelrate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "aS9-oCGXURNO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load dataset"
      ],
      "metadata": {
        "id": "ikGEAcUWWCYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset('wikiann','ko')\n",
        "label_names = data['train'].features['ner_tags'].feature.names\n"
      ],
      "metadata": {
        "id": "jnYQ_FuyVZtc"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2label = {i : l for i, l in enumerate(label_names)}\n",
        "idx2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohz03bR8n7kc",
        "outputId": "a1e72d06-f434-4cc4-d1d6-52c177852590"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-PER',\n",
              " 2: 'I-PER',\n",
              " 3: 'B-ORG',\n",
              " 4: 'I-ORG',\n",
              " 5: 'B-LOC',\n",
              " 6: 'I-LOC'}"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape, label_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd3eRnuSVqUk",
        "outputId": "8b18effd-6fbe-4986-d478-300f923e5271"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'validation': (10000, 4), 'test': (10000, 4), 'train': (20000, 4)},\n",
              " ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# token과 ner-tag\n",
        "data['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nyzXKmtVv8o",
        "outputId": "60552a2e-b396-4e00-91ee-0f7ab391573a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
              "    num_rows: 20000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWTAXucUV28h",
        "outputId": "9fa959ab-7f29-4efa-e18c-e596b0167396"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokens': ['현재',\n",
              "  '대한민국',\n",
              "  'K리그',\n",
              "  '챌린지의',\n",
              "  '서울',\n",
              "  '이랜드',\n",
              "  'FC에서',\n",
              "  '활약하고',\n",
              "  '있다',\n",
              "  '.'],\n",
              " 'ner_tags': [0, 5, 3, 4, 3, 4, 4, 0, 0, 0],\n",
              " 'langs': ['ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko'],\n",
              " 'spans': ['LOC: 대한민국', 'ORG: K리그 챌린지의', 'ORG: 서울 이랜드 FC에서']}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load model : koelectra\n",
        ": pretrained ELECTRA language model 한국어판\n",
        "\n",
        "- 생성기에서 토큰을 보고 판별기에서 그것이 '진짜' 토큰인지 '가짜 ' 토큰인지 판단하여 학습함\n",
        "- 이 방법을 사용할 경우, 모든 입력 토큰을 훈련할 수 있으며, bert 등과 비교하여 효과가 우수함\n",
        "- WordPiece를 사용\n",
        "\n",
        "\n",
        "\n",
        "(참고자료)\n",
        "- 링크 1 : https://huggingface.co/monologg/koelectra-base-v3-generator\n",
        "- 링크 2 : https://github.com/monologg/KoELECTRA/blob/master/README_EN.md\n",
        "- 링크 3 : https://github.com/monologg/KoELECTRA/blob/master/finetune/README_EN.md"
      ],
      "metadata": {
        "id": "fXjXlZxFV-sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"monologg/koelectra-small-v3-discriminator\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
        "                                                       num_labels = len(label_names)).to('cuda')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cArBDcAvV5b1",
        "outputId": "cb1c100e-4933-4c71-c2bf-fed5a601ff1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['train'][0])\n",
        "text =' '.join(data['train']['tokens'][0])\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHEb2i6KWK1N",
        "outputId": "e73401fc-6ead-4990-f1de-15b89e792fe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tokens': ['현재', '대한민국', 'K리그', '챌린지의', '서울', '이랜드', 'FC에서', '활약하고', '있다', '.'], 'ner_tags': [0, 5, 3, 4, 3, 4, 4, 0, 0, 0], 'langs': ['ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko'], 'spans': ['LOC: 대한민국', 'ORG: K리그 챌린지의', 'ORG: 서울 이랜드 FC에서']}\n",
            "[2, 6339, 7001, 47, 19611, 18756, 4234, 6265, 23358, 10839, 4073, 4129, 8377, 4279, 4219, 3249, 4176, 18, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch 단위로 인코딩 하는 함수\n",
        "batch_result_line2 = tokenizer.batch_encode_plus(data['train']['tokens'][:3], is_split_into_words = True)\n",
        "batch_result_line2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9myiiHkWMA5",
        "outputId": "40da7150-abdd-4e51-f768-1103bdefc242"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[2, 6339, 7001, 47, 19611, 18756, 4234, 6265, 23358, 10839, 4073, 4129, 8377, 4279, 4219, 3249, 4176, 18, 3], [2, 11, 11, 11, 3598, 25892, 11, 11, 11, 3], [2, 11554, 4172, 4162, 2780, 4031, 3311, 4112, 2241, 15, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids, token_type_ids, attention_mask를 위한 값 생성\n",
        "\n",
        "def tokenize_adjust_labels(all_samples):\n",
        "  tokenized_samples = tokenizer.batch_encode_plus(all_samples['tokens'],\n",
        "                                                  is_split_into_words = True)\n",
        "\n",
        "  total_adjusted_labels = []\n",
        "  for k in range(0, len(tokenized_samples['input_ids'])):\n",
        "    prev_wid = -1\n",
        "    word_id_list = tokenized_samples.word_ids(batch_index = k) # output # 같은 단어는 같은 id : [None, 0, 1, 2, 2, 3, 3, 4, 5, 6, 6, 6, 7, 7, 7, 8, 8, 9, None]\n",
        "    existing_label_ids = all_samples['ner_tags'][k] # output : [[0, 5, 3, 4, 3, 4, 4, 0, 0, 0],...]\n",
        "    i = -1\n",
        "    adjusted_label_ids = []\n",
        "\n",
        "    for wid in word_id_list:\n",
        "      if wid is None: # 단어 리스트에 포함되지 않으면 -100\n",
        "        adjusted_label_ids.append(-100)\n",
        "\n",
        "\n",
        "      elif wid != prev_wid: # 새 단어가 등장하여 과거 word_id 와 다를 경우\n",
        "        i += 1\n",
        "        adjusted_label_ids.append(existing_label_ids[i])\n",
        "        prev_wid = wid\n",
        "      else: # 등장하기 시작한 word_id 가 유지될 경우\n",
        "        label_name = label_names[existing_label_ids[i]]\n",
        "        adjusted_label_ids.append(existing_label_ids[i])\n",
        "\n",
        "    total_adjusted_labels.append(adjusted_label_ids)\n",
        "  tokenized_samples['labels'] = total_adjusted_labels\n",
        "  return tokenized_samples\n"
      ],
      "metadata": {
        "id": "eZj8tjPqWmDy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = data.map(tokenize_adjust_labels, batched = True)"
      ],
      "metadata": {
        "id": "vgCpNNFBW2lu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWja1oZUouEU",
        "outputId": "79d5c3dd-66c0-45eb-cbe3-43d718d454a8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokens': ['현재',\n",
              "  '대한민국',\n",
              "  'K리그',\n",
              "  '챌린지의',\n",
              "  '서울',\n",
              "  '이랜드',\n",
              "  'FC에서',\n",
              "  '활약하고',\n",
              "  '있다',\n",
              "  '.'],\n",
              " 'ner_tags': [0, 5, 3, 4, 3, 4, 4, 0, 0, 0],\n",
              " 'langs': ['ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko'],\n",
              " 'spans': ['LOC: 대한민국', 'ORG: K리그 챌린지의', 'ORG: 서울 이랜드 FC에서'],\n",
              " 'input_ids': [2,\n",
              "  6339,\n",
              "  7001,\n",
              "  47,\n",
              "  19611,\n",
              "  18756,\n",
              "  4234,\n",
              "  6265,\n",
              "  23358,\n",
              "  10839,\n",
              "  4073,\n",
              "  4129,\n",
              "  8377,\n",
              "  4279,\n",
              "  4219,\n",
              "  3249,\n",
              "  4176,\n",
              "  18,\n",
              "  3],\n",
              " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'labels': [-100, 0, 5, 3, 3, 4, 4, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, -100]}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data # 이전"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsdwQhGzXO2P",
        "outputId": "29c85e5b-cb61-4b74-ce1a-4fcbc60363f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    validation: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    train: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset # tokenizer.tokenize()를 통과시켰을 때 및 label이 추가됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHDmiILYXPH6",
        "outputId": "e1bc2fd3-ea44-4099-cad6-757c368e2bba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    validation: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    train: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "Fw4n41-JXu-E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval\n",
        "\n",
        "metric = load_metric('seqeval')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOce2qeMXu7D",
        "outputId": "b0fb2948-7ae4-45ad-fcd4-9f5ae4fc4a85"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-fb2632c3f8b2>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('seqeval')\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "  predictions, labels = pred\n",
        "  predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "  # speical token은 삭제\n",
        "  true_predictions = [\n",
        "      [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "       for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "\n",
        "  true_labels = [\n",
        "      [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "       for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "\n",
        "  # metrics.compute\n",
        "  results = metric.compute(predictions = true_predictions, references = true_labels)\n",
        "  flattened_results = {\n",
        "        \"overall_precision\": results[\"overall_precision\"],\n",
        "        \"overall_recall\": results[\"overall_recall\"],\n",
        "        \"overall_f1\": results[\"overall_f1\"],\n",
        "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
        "  }\n",
        "\n",
        "  for k in results.keys():\n",
        "    if k not in flattened_results.keys():\n",
        "      flattened_results[f'{k}_f1'] = results[k]['f1']\n",
        "\n",
        "  return flattened_results"
      ],
      "metadata": {
        "id": "eQguLK_sXu37"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Trainer 를 활용하여 학습하기"
      ],
      "metadata": {
        "id": "w0SZmCxUn9ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 추상화된 객체 사용해서 학습\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = './find_tune_bert_output',\n",
        "    evaluation_strategy='steps', # 훈련중에 채택할 평가전략 (no, steps, epoch)\n",
        "    learning_rate = 2e-5,\n",
        "    per_device_train_batch_size = 16,\n",
        "    per_device_eval_batch_size = 16,\n",
        "    num_train_epochs = 7,\n",
        "    weight_decay = 0.01,\n",
        "    logging_steps = 1000,\n",
        "    save_strategy = 'no', # 훈련중에는 저장하지 않음,\n",
        "    use_cpu = False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_dataset['train'],\n",
        "    eval_dataset = tokenized_dataset['validation'],\n",
        "    data_collator = data_collator,\n",
        "    tokenizer = tokenizer,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "Gi9PsY1JXu0z",
        "outputId": "320a7db3-70e4-4114-c615-d6d9dc6bbb9c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5605' max='8750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5605/8750 05:18 < 02:58, 17.60 it/s, Epoch 4.48/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Precision</th>\n",
              "      <th>Overall Recall</th>\n",
              "      <th>Overall F1</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Loc F1</th>\n",
              "      <th>Org F1</th>\n",
              "      <th>Per F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.214600</td>\n",
              "      <td>0.290305</td>\n",
              "      <td>0.763195</td>\n",
              "      <td>0.834043</td>\n",
              "      <td>0.797048</td>\n",
              "      <td>0.917495</td>\n",
              "      <td>0.872935</td>\n",
              "      <td>0.681205</td>\n",
              "      <td>0.796561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.253800</td>\n",
              "      <td>0.237364</td>\n",
              "      <td>0.818328</td>\n",
              "      <td>0.828968</td>\n",
              "      <td>0.823614</td>\n",
              "      <td>0.929880</td>\n",
              "      <td>0.894029</td>\n",
              "      <td>0.718694</td>\n",
              "      <td>0.817802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.219200</td>\n",
              "      <td>0.226512</td>\n",
              "      <td>0.824571</td>\n",
              "      <td>0.843662</td>\n",
              "      <td>0.834007</td>\n",
              "      <td>0.933968</td>\n",
              "      <td>0.895636</td>\n",
              "      <td>0.740318</td>\n",
              "      <td>0.832225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.196700</td>\n",
              "      <td>0.220646</td>\n",
              "      <td>0.826071</td>\n",
              "      <td>0.852936</td>\n",
              "      <td>0.839288</td>\n",
              "      <td>0.935432</td>\n",
              "      <td>0.904700</td>\n",
              "      <td>0.749934</td>\n",
              "      <td>0.829020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.181400</td>\n",
              "      <td>0.216909</td>\n",
              "      <td>0.837071</td>\n",
              "      <td>0.853750</td>\n",
              "      <td>0.845328</td>\n",
              "      <td>0.937919</td>\n",
              "      <td>0.905687</td>\n",
              "      <td>0.760548</td>\n",
              "      <td>0.838792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8750' max='8750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8750/8750 08:10, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Precision</th>\n",
              "      <th>Overall Recall</th>\n",
              "      <th>Overall F1</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Loc F1</th>\n",
              "      <th>Org F1</th>\n",
              "      <th>Per F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.214600</td>\n",
              "      <td>0.290305</td>\n",
              "      <td>0.763195</td>\n",
              "      <td>0.834043</td>\n",
              "      <td>0.797048</td>\n",
              "      <td>0.917495</td>\n",
              "      <td>0.872935</td>\n",
              "      <td>0.681205</td>\n",
              "      <td>0.796561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.253800</td>\n",
              "      <td>0.237364</td>\n",
              "      <td>0.818328</td>\n",
              "      <td>0.828968</td>\n",
              "      <td>0.823614</td>\n",
              "      <td>0.929880</td>\n",
              "      <td>0.894029</td>\n",
              "      <td>0.718694</td>\n",
              "      <td>0.817802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.219200</td>\n",
              "      <td>0.226512</td>\n",
              "      <td>0.824571</td>\n",
              "      <td>0.843662</td>\n",
              "      <td>0.834007</td>\n",
              "      <td>0.933968</td>\n",
              "      <td>0.895636</td>\n",
              "      <td>0.740318</td>\n",
              "      <td>0.832225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.196700</td>\n",
              "      <td>0.220646</td>\n",
              "      <td>0.826071</td>\n",
              "      <td>0.852936</td>\n",
              "      <td>0.839288</td>\n",
              "      <td>0.935432</td>\n",
              "      <td>0.904700</td>\n",
              "      <td>0.749934</td>\n",
              "      <td>0.829020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.181400</td>\n",
              "      <td>0.216909</td>\n",
              "      <td>0.837071</td>\n",
              "      <td>0.853750</td>\n",
              "      <td>0.845328</td>\n",
              "      <td>0.937919</td>\n",
              "      <td>0.905687</td>\n",
              "      <td>0.760548</td>\n",
              "      <td>0.838792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.166100</td>\n",
              "      <td>0.216868</td>\n",
              "      <td>0.840269</td>\n",
              "      <td>0.848957</td>\n",
              "      <td>0.844591</td>\n",
              "      <td>0.938178</td>\n",
              "      <td>0.904957</td>\n",
              "      <td>0.758254</td>\n",
              "      <td>0.837846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.157700</td>\n",
              "      <td>0.211649</td>\n",
              "      <td>0.841288</td>\n",
              "      <td>0.860424</td>\n",
              "      <td>0.850748</td>\n",
              "      <td>0.940121</td>\n",
              "      <td>0.908809</td>\n",
              "      <td>0.766986</td>\n",
              "      <td>0.847248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.154400</td>\n",
              "      <td>0.211022</td>\n",
              "      <td>0.846809</td>\n",
              "      <td>0.856069</td>\n",
              "      <td>0.851414</td>\n",
              "      <td>0.940529</td>\n",
              "      <td>0.909014</td>\n",
              "      <td>0.768458</td>\n",
              "      <td>0.847706</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8750, training_loss=0.18901681256975447, metrics={'train_runtime': 490.969, 'train_samples_per_second': 285.15, 'train_steps_per_second': 17.822, 'total_flos': 342151515102240.0, 'train_loss': 0.18901681256975447, 'epoch': 7.0})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ssy9LOJjc4k",
        "outputId": "d2d5153e-8010-4fd5-e8f8-bafa0e305257"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset_result =trainer.predict(tokenized_dataset['test'])\n",
        "testset_result[0].shape # 10_000개가 164 토크나이징 되어 7개의 ner tag 를 가지고 있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vSeT4WFptM4M",
        "outputId": "5f506cd7-00c4-4685-f0fc-0d5c0c4633f0"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxpPK_6ZznR3",
        "outputId": "dacc5af0-0126-459f-f6a3-d183b3c5f6b9"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.trainer.Trainer at 0x7a3606b8eaa0>"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 기반 결과물\n",
        "[idx2label[i] for i in testset_result[0][0].argmax(axis=1)][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUnZ1F-st_tp",
        "outputId": "dd74a93b-fc21-40de-b082-c3d86f9bd844"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'I-PER', 'O', 'O', 'O']"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model architecture\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E72eHwzzy0T",
        "outputId": "4f4bc48c-d72d-49aa-cb11-f0f335067f75"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElectraForTokenClassification(\n",
              "  (electra): ElectraModel(\n",
              "    (embeddings): ElectraEmbeddings(\n",
              "      (word_embeddings): Embedding(35000, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
              "    (encoder): ElectraEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=256, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model save"
      ],
      "metadata": {
        "id": "HWlBCu_gna_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "\n",
        "# 1) trainer 를 사용하여 업로드하기\n",
        "#trainer.push_to_hub() # output_dir 에 지정한 경로명이 반영\n",
        "\n",
        "# 2) 모델을 로컬경로에 저장한 다음 업로드하기\n",
        "\n",
        "path = 'koelectra-small-v3-discriminator-ft-wikiann-ko-ner'\n",
        "trainer.save_model(f'./{path}')\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained('./koelectra-small-v3-discriminator-ft-wikiann-ko-ner')\n",
        "model = AutoModelForTokenClassification.from_pretrained('./koelectra-small-v3-discriminator-ft-wikiann-ko-ner')\n",
        "\n",
        "#idx2label[7] = 'NONE' # 앞뒤 -100을 넣어준 부분을 label dictionary 에도 동일하게 반영\n",
        "model.config.id2label= idx2label # ner tag 추가\n",
        "\n",
        "model.push_to_hub(path)\n",
        "tokenizer.push_to_hub(path)"
      ],
      "metadata": {
        "id": "GD6xx7vnyenW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained('seriouspark/koelectra-small-v3-discriminator-ft-wikiann-ko-ner')\n",
        "model = AutoModelForTokenClassification.from_pretrained('seriouspark/koelectra-small-v3-discriminator-ft-wikiann-ko-ner')\n",
        "\n",
        "# 문장 하나를 넣어 확인\n",
        "try_text = '아침에 일어나 학교를 간 김고은'\n",
        "inputs = tokenizer([try_text], return_tensors = 'pt')\n",
        "with torch.no_grad():\n",
        "  logits = model(**inputs)\n",
        "result_line =np.argmax(logits.logits[0], axis=1)\n",
        "\n",
        "# ner 결과물을 실제로 확인\n",
        "text_tokens = tokenizer.tokenize(try_text)\n",
        "ner_tag = [idx2label[i.item()] for i in result_line][1:-1]\n",
        "for tokens, tag in zip(text_tokens,ner_tag ):\n",
        "  print(tokens, tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215,
          "referenced_widgets": [
            "d6b26eee96364222af8d4559006f1872",
            "ea5a04ddf4dc450da9e403b4b947cb55",
            "efa848e16857481d8d2dae84ddb2b4c4",
            "8b7c9f99222b4a3789baad3345c3a39e",
            "6ab0f5b729164334a2a6cef4885b1601",
            "d33745f8f9c94113b374c146d2db79ad",
            "1a8115de02984c48849bfe417ec59ccb",
            "2545f44262264dc49b21d113c3c61aee",
            "c80dcaa7c91941cf87dbdc6b0c2f4fbf",
            "a0bb4436c6fb454e9d21b13c5061bde4",
            "1ec61c0d684b4e22acd57c185f2dcec5"
          ]
        },
        "id": "Khpx7xStskrt",
        "outputId": "b3cab22e-e1a5-4675-ca6b-0a2bf95e3914"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6b26eee96364222af8d4559006f1872"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아침 O\n",
            "##에 O\n",
            "일어나 O\n",
            "학교 O\n",
            "##를 O\n",
            "간 O\n",
            "김 B-PER\n",
            "##고 B-PER\n",
            "##은 B-PER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. training loop 직접 구현하기"
      ],
      "metadata": {
        "id": "ST1J1l9yn2e_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "07RsBvv4ofPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOBt2-uDohan",
        "outputId": "4f912e32-a878-4914-ceed-ae0d766ddf1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize"
      ],
      "metadata": {
        "id": "i4GshBEMzrAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = \"monologg/koelectra-small-v3-discriminator\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "P-kj8Y7onxvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load nsmc dataset\n",
        "dataset = load_dataset('nsmc')\n",
        "\n",
        "# get label info\n",
        "print(set(dataset['train']['label']))\n",
        "\n",
        "#defind tokenizing function\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples['document'], padding = 'max_length', truncation = True)\n",
        "\n",
        "# tokenize dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched = True)"
      ],
      "metadata": {
        "id": "pTT8OtlWp3tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model + additional layers\n",
        "\n",
        "class StackMoreLayers(nn.Module):\n",
        "  def __init__(self, model_name):\n",
        "    super(StackMoreLayers, self).__init__()\n",
        "    self.encoder = AutoModel.from_pretrained(model_name)\n",
        "    self.lstm = nn.LSTM(input_size = self.encoder.config.hidden_size,\n",
        "                        hidden_size = 512,\n",
        "                        batch_first = True, # batch,\n",
        "                        bidirectional = True)\n",
        "    self.interaction_head = nn.Linear(1024, 1) # 512 & bidirection 이기 때문에 1024, 1 의 shape 을 가짐\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.encoder(input_ids=input_ids, attention_mask = attention_mask)\n",
        "    #print(outputs.shape)\n",
        "    last_hidden_state = outputs.last_hidden_state\n",
        "    #print(last_hidden_state.shape)\n",
        "    outputs = self.lstm(last_hidden_state)\n",
        "    #print(outputs.shape)\n",
        "    outputs = self.interaction_head(outputs[0])\n",
        "    #print(outputs.shape)\n",
        "    return outputs\n",
        "\n",
        "# model + additional layers\n",
        "\n",
        "class StackMoreLayers2(nn.Module):\n",
        "  def __init__(self, model_name):\n",
        "    super(StackMoreLayers2, self).__init__()\n",
        "    self.encoder = AutoModel.from_pretrained(model_name)\n",
        "    self.additional_layer = AutoModel.from_pretrained(model_name)\n",
        "    self.lstm = nn.LSTM(input_size = self.encoder.config.hidden_size,\n",
        "                        hidden_size = 512,\n",
        "                        batch_first = True, # batch,\n",
        "                        bidirectional = True)\n",
        "    self.interaction_head = nn.Linear(1024, 1) # 512 & bidirection 이기 때문에 1024, 1 의 shape 을 가짐\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.encoder(input_ids=input_ids, attention_mask = attention_mask)\n",
        "    #print(outputs.shape)\n",
        "    last_hidden_state = outputs.last_hidden_state\n",
        "    #print(last_hidden_state.shape)\n",
        "    outputs = self.lstm(last_hidden_state)\n",
        "    #print(outputs.shape)\n",
        "    outputs = self.interaction_head(outputs[0])\n",
        "    #print(outputs.shape)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "eGsORingoRM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.rename_column('label','labels')\n",
        "# 아래 부분 없이 dataset 자체에서 처리하게 되면, batch단위 tensor 를 만들 수 없엉진다.\n",
        "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle = True, batch_size = 4)\n",
        "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size = 4)"
      ],
      "metadata": {
        "id": "HupEWNErpvAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr = 5e-5)\n",
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "#loss_fn = nn.BCELoss()\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "I6W4sp3QrthC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding 과 embedding_project 가 Electra 모델 구조에서 2번 사용되는 이유? 각각 다른 역할을 수행하기 때문\n",
        "\n",
        "1. Embedding : 모델의 입력 토큰을 벡터 형태로 변환하는 역할을 수행\n",
        "  - 단어 ID에 해당하는 임베딩 벡터를 찾아와 모델에 입력\n",
        "  - Electra 모델의 electraEmbeddings 레이어에는 단어 임베딩, 위치 임베딩, 토큰 타입 임베딩 등이 포함됨\n",
        "  - 이런 임베딩은 모델에 입력되기 전에 적절한 차원으로 변환\n",
        "\n",
        "2. EmbeddingProject : 임베딩 레이어의 출력을 더 큰 차원으로 변환하는 역할을 함\n",
        "  - Electra 모델의 embedding_project 레이어는 Embedding 레이어의 출력 벡터차원을 더 큰 차원으로 확장\n",
        "  - 모델이 더 복잡한 틁징을 학습하고 더 깊은 이해를 할 수 있음\n",
        "  - 모델의 용량을 늘리고 더 복잡한 관계 학습이 가능"
      ],
      "metadata": {
        "id": "PZF53hsJyfB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, input_size = (128,))  # 기존 electra model"
      ],
      "metadata": {
        "id": "ZhlzWL35vgxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = StackMoreLayers(model_name).to(device)\n",
        "new_model2 = StackMoreLayers2(model_name).to(device)"
      ],
      "metadata": {
        "id": "3gPQZSNttGnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문 : electraencoder 가 다 날라간건가?\n",
        "summary(new_model, input_size = (128,))   # 기존 electra model + LStM + interaction head"
      ],
      "metadata": {
        "id": "EaCTneFBsvL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "vAobLE1U9aci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 레이어를 freeze\n",
        "print('임베딩 레이어 freeze')\n",
        "print(new_model.encoder.embeddings)\n",
        "for param in new_model.encoder.embeddings.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "print('-'* 10)\n",
        "\n",
        "\n",
        "# 첫번째 어텐션 레이어 freeze\n",
        "print('첫번째 어텐션 레이어 freeze')\n",
        "print(new_model.encoder.encoder.layer[0])\n",
        "for param in new_model.encoder.encoder.layer[0].parameters():\n",
        "  param.requires_grad = False\n",
        "# 수정코드\n",
        "for param in new_model.encoder.encoder.layer[0].attention.self.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "## 원본코드\n",
        "#for param in new_model.encoder.encoder.layer[1].attention.self.parameters():\n",
        "#  param.requires_grad = False"
      ],
      "metadata": {
        "id": "XMFSjg_Mr0G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model"
      ],
      "metadata": {
        "id": "RqbMu15u-Nyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## training\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for batch in train_dataloader:\n",
        "    target = batch['label'].to(device)\n",
        "    batch = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
        "\n",
        "    print(batch.keys())\n",
        "    outputs = new_model(**batch).to(device)\n",
        "    loss = criterion(outputs[:, -1].squeeze(1), target.float())\n",
        "    loss = outputs.loss\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad() # 그래디언트 초기화, 없다면 이전 step부터 누적\n",
        "    loss.backward()       # backprop 으로 그래디언트 계산\n",
        "    optimizer.step()      # 계산한 그래디언트로 가중치를 조정\n",
        "\n",
        "  avg_train_loss = total_loss / len(train_dataloader)\n",
        "  print(f'Epoch {epoch +1} / {num_epochs} | Train Loss : {avg_train_loss}')\n",
        "\n",
        "  # 검증루프\n",
        "  model.eval() # eval 모드, 추론을 위해 dropout 등의 기능은 제외\n",
        "  total_eval_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in eval_dataloader:\n",
        "      batch = {k : v.to(device) for k, v in batch.item()}\n",
        "      outputs = model(**batch)\n",
        "      loss = outputs.loss\n",
        "      total_eval_loss += loss.item()\n",
        "\n",
        "  avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "  print(f'Epoch {epoch+1} / {num_epochs} | Eval Loss : {avg_eval_loss}')\n",
        "\n",
        "# 추가 head 는 저장이 되지 않기 때문에 safetensors 로 저장\n",
        "model.encoder.save_pretrained(f'model_{epoch+1}')\n",
        "# 추가 레이어의 weight까지 저장하려면 .pt 파일로 저장 가능하다.\n",
        "import os\n",
        "os.makedirs('./koelectra-lstm/', exist_ok = True)\n",
        "torch.save(model.lstm.state_dict(), \"model.pt\")"
      ],
      "metadata": {
        "id": "cNxOCIAd2aAY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}